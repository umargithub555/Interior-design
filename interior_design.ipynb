{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba89676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d68c740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0f7031b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a160d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "c:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `hf_hub_download`. Downloading to a local directory does not use symlinks anymore.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "black-forest-labs/FLUX.1-dev is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `token` or log in with `hf auth login`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:657\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\httpx\\_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '401 Unauthorized' for url 'https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/model_index.json'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGatedRepoError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\diffusers\\configuration_utils.py:392\u001b[39m, in \u001b[36mConfigMixin.load_config\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, return_unused_kwargs, return_commit_hash, **kwargs)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    391\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m     config_file = \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:89\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m kwargs = smoothly_deprecate_legacy_arguments(fn_name=fn.\u001b[34m__name__\u001b[39m, kwargs=kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\huggingface_hub\\file_download.py:1024\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, etag_timeout, token, local_files_only, headers, endpoint, tqdm_class, dry_run)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1038\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\huggingface_hub\\file_download.py:1175\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, token, local_files_only, force_download, tqdm_class, dry_run)\u001b[39m\n\u001b[32m   1174\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m         \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\huggingface_hub\\file_download.py:1804\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1799\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1800\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error.response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m   1801\u001b[39m ):\n\u001b[32m   1802\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m   1803\u001b[39m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1804\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1805\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1806\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\huggingface_hub\\file_download.py:1691\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder, retry_on_errors)\u001b[39m\n\u001b[32m   1690\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1691\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_on_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_on_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1698\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1699\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RemoteEntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:89\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m kwargs = smoothly_deprecate_legacy_arguments(fn_name=fn.\u001b[34m__name__\u001b[39m, kwargs=kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\huggingface_hub\\file_download.py:1614\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, timeout, library_name, library_version, user_agent, headers, endpoint, retry_on_errors)\u001b[39m\n\u001b[32m   1613\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1614\u001b[39m response = \u001b[43m_httpx_follow_relative_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1615\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_on_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_on_errors\u001b[49m\n\u001b[32m   1616\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1617\u001b[39m hf_raise_for_status(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\huggingface_hub\\file_download.py:309\u001b[39m, in \u001b[36m_httpx_follow_relative_redirects\u001b[39m\u001b[34m(method, url, retry_on_errors, **httpx_kwargs)\u001b[39m\n\u001b[32m    302\u001b[39m response = http_backoff(\n\u001b[32m    303\u001b[39m     method=method,\n\u001b[32m    304\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m     **no_retry_kwargs,\n\u001b[32m    308\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;66;03m# Check if response is a relative redirect\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:677\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    674\u001b[39m     message = (\n\u001b[32m    675\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    676\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_message == \u001b[33m\"\u001b[39m\u001b[33mAccess to this resource is disabled.\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mGatedRepoError\u001b[39m: 401 Client Error. (Request ID: Root=1-693a4822-3b25083961e6610e56fe47ec;047bca21-49e9-44c3-b753-998128777ddb)\n\nCannot access gated repo for url https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/model_index.json.\nAccess to model black-forest-labs/FLUX.1-dev is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdiffusers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoPipelineForText2Image\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m pipeline = \u001b[43mAutoPipelineForText2Image\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mblack-forest-labs/FLUX.1-dev\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m.to(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m pipeline.load_lora_weights(\u001b[33m'\u001b[39m\u001b[33mViktor1717/scandinavian-interior-style\u001b[39m\u001b[33m'\u001b[39m, weight_name=\u001b[33m'\u001b[39m\u001b[33mmy_first_flux_lora_v1.safetensors\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:89\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     85\u001b[39m         validate_repo_id(arg_value)\n\u001b[32m     87\u001b[39m kwargs = smoothly_deprecate_legacy_arguments(fn_name=fn.\u001b[34m__name__\u001b[39m, kwargs=kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\diffusers\\pipelines\\auto_pipeline.py:465\u001b[39m, in \u001b[36mAutoPipelineForText2Image.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_or_path, **kwargs)\u001b[39m\n\u001b[32m    454\u001b[39m revision = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mrevision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    456\u001b[39m load_config_kwargs = {\n\u001b[32m    457\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcache_dir\u001b[39m\u001b[33m\"\u001b[39m: cache_dir,\n\u001b[32m    458\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mforce_download\u001b[39m\u001b[33m\"\u001b[39m: force_download,\n\u001b[32m   (...)\u001b[39m\u001b[32m    462\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrevision\u001b[39m\u001b[33m\"\u001b[39m: revision,\n\u001b[32m    463\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m config = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mload_config_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m orig_class_name = config[\u001b[33m\"\u001b[39m\u001b[33m_class_name\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mControlPipeline\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m orig_class_name:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:89\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     85\u001b[39m         validate_repo_id(arg_value)\n\u001b[32m     87\u001b[39m kwargs = smoothly_deprecate_legacy_arguments(fn_name=fn.\u001b[34m__name__\u001b[39m, kwargs=kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\diffusers\\configuration_utils.py:407\u001b[39m, in \u001b[36mConfigMixin.load_config\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, return_unused_kwargs, return_commit_hash, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m     config_file = hf_hub_download(\n\u001b[32m    393\u001b[39m         pretrained_model_name_or_path,\n\u001b[32m    394\u001b[39m         filename=\u001b[38;5;28mcls\u001b[39m.config_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    404\u001b[39m         local_dir_use_symlinks=local_dir_use_symlinks,\n\u001b[32m    405\u001b[39m     )\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    408\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a local folder and is not a valid model identifier\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    409\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m listed on \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mIf this is a private repository, make sure to pass a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    410\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m token having permission to this repo with `token` or log in with `hf auth login`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    411\u001b[39m     )\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError:\n\u001b[32m    413\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    414\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a valid git identifier (branch name, tag name or commit id) that exists for\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m this model name. Check the model page at\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    416\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m for available revisions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m     )\n",
      "\u001b[31mOSError\u001b[39m: black-forest-labs/FLUX.1-dev is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `token` or log in with `hf auth login`."
     ]
    }
   ],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "import torch\n",
    "\n",
    "pipeline = AutoPipelineForText2Image.from_pretrained('black-forest-labs/FLUX.1-dev', torch_dtype=torch.bfloat16).to('cuda')\n",
    "pipeline.load_lora_weights('Viktor1717/scandinavian-interior-style', weight_name='my_first_flux_lora_v1.safetensors')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832b7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipeline('[trigger] living room with natural light, wood floors, minimalistic furniture').images[0]\n",
    "image.save(\"my_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8e82cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from requests->transformers) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from requests->transformers) (2025.11.12)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/12.0 MB 11.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.7/12.0 MB 11.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.3/12.0 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.7/12.0 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.3/12.0 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 11.5 MB/s  0:00:01\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
      "\n",
      "  Attempting uninstall: huggingface-hub\n",
      "\n",
      "    Found existing installation: huggingface_hub 1.2.2\n",
      "\n",
      "    Uninstalling huggingface_hub-1.2.2:\n",
      "\n",
      "      Successfully uninstalled huggingface_hub-1.2.2\n",
      "\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   ---------------------------------------- 3/3 [transformers]\n",
      "\n",
      "Successfully installed huggingface-hub-0.36.0 tokenizers-0.22.1 transformers-4.57.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb4117a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (0.36.0)\n",
      "Requirement already satisfied: filelock in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from huggingface_hub) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from huggingface_hub) (2025.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from huggingface_hub) (6.0.3)\n",
      "Requirement already satisfied: requests in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from huggingface_hub) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from requests->huggingface_hub) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from requests->huggingface_hub) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from requests->huggingface_hub) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from requests->huggingface_hub) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc37f28d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1355118443.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mhuggingface-cli login\u001b[39m\n                    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e0c65b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f85d674e0e44b21bdf1622c2c0705ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a78b6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222f80e93c6c46b8a8111df5cd13f410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/536 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\IR - CP 15\\.cache\\huggingface\\hub\\models--black-forest-labs--FLUX.1-dev. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601d1e8a79ab4771bc2061b9c340358a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 23 files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445f98d7282a4ac3b26ea51ee228a02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler_config.json:   0%|          | 0.00/273 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9747bb9a37f43a1914dbad1aaefbac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c06a3d6cec249d48eb125004615390b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/782 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1954003f564fb794e6f870574bf8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3fbf665ba14b21b8a870a8a92ae410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder_2/model-00001-of-00002.safe(â€¦):   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b35415c6204b25bc99d30b014620ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/model.safetensors:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76374dbb0d24dbe9d23231e08061c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder_2/model-00002-of-00002.safe(â€¦):   0%|          | 0.00/4.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77f677eaedc42cb9f88beb1a257e87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/19.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6d09788c06467bb5ee69101bc304b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/588 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7772398c945547a9b499dff82795a85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/705 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c357d47f4e584e9394a370971f70c98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254b4f5d32954ac48b4cd10496cfb68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6258d1500a53433ebee4875acd7813da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_2/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98d5f9c287a445593e14694495fd0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6628f26f1c9946a28114ea50031c1805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b044df3dbb445aae7b85087c9a2bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/378 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a2a4c2978d485198b6cfeb0f7c4e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transformer/diffusion_pytorch_model-0000(â€¦):   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c3f8f6f8a74e4b99874b51f94d0679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transformer/diffusion_pytorch_model-0000(â€¦):   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7e4ffb31a849379c7fd51a5a67ff75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transformer/diffusion_pytorch_model-0000(â€¦):   0%|          | 0.00/3.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50629053c4d4097871403948709a17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)ion_pytorch_model.safetensors.index.json:   0%|          | 0.00/121k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494508faac4749998dd2aa0f8351c2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/820 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d182a1bff44345fcb149acfbe0a6b838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/diffusion_pytorch_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hugging_face_token = os.getenv(\"Hugging_face_token\")\n",
    "\n",
    "\n",
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16, use_auth_token=hugging_face_token)\n",
    "pipe.enable_model_cpu_offload() #save some VRAM by offloading the model to CPU. Remove this if you have enough GPU power\n",
    "\n",
    "prompt = \"A cat holding a sign that says hello world\"\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    guidance_scale=3.5,\n",
    "    num_inference_steps=50,\n",
    "    max_sequence_length=512,\n",
    "    generator=torch.Generator(\"cpu\").manual_seed(0)\n",
    ").images[0]\n",
    "image.save(\"flux-dev.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e92615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cee7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a678b5f85146508c587b4302fafaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 23 files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824522659eed4b219e487a9593908a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/model.safetensors:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504dc4d65b034024bf6f963dfbefed12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder_2/model-00002-of-00002.safe(â€¦):   0%|          | 0.00/4.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96bb814139964f0f89e9884a441395ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder_2/model-00001-of-00002.safe(â€¦):   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94bc53d2ffa489f8a1d2bfb692716ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/diffusion_pytorch_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af57deb0ec94fb2bd71867c67257ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transformer/diffusion_pytorch_model-0000(â€¦):   0%|          | 0.00/3.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85a7fc25e5942a6ab9015f59b2d9d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transformer/diffusion_pytorch_model-0000(â€¦):   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b71778e1664e7a9095b362bbf7995d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transformer/diffusion_pytorch_model-0000(â€¦):   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "import torch\n",
    "\n",
    "pipeline = AutoPipelineForText2Image.from_pretrained('black-forest-labs/FLUX.1-dev', torch_dtype=torch.bfloat16).to('cuda')\n",
    "pipeline.load_lora_weights('Viktor1717/scandinavian-interior-style', weight_name='my_first_flux_lora_v1.safetensors')\n",
    "image = pipeline('[trigger] living room with natural light, wood floors, minimalistic furniture').images[0]\n",
    "image.save(\"my_image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0df3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/diffusers 'C:\\Users\\IR - CP 15\\AppData\\Local\\Temp\\pip-req-build-11jf3i4y'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/diffusers\n",
      "  Cloning https://github.com/huggingface/diffusers to c:\\users\\ir - cp 15\\appdata\\local\\temp\\pip-req-build-11jf3i4y\n",
      "  Resolved https://github.com/huggingface/diffusers to commit 6708f5c76d50be208b8043c58e142d6551e4fba5\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: importlib_metadata in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from diffusers==0.36.0.dev0) (8.7.0)\n",
      "Requirement already satisfied: filelock in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from diffusers==0.36.0.dev0) (3.20.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from diffusers==0.36.0.dev0) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.34.0 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from diffusers==0.36.0.dev0) (0.36.0)\n",
      "Requirement already satisfied: numpy in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from diffusers==0.36.0.dev0) (2.3.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from diffusers==0.36.0.dev0) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from diffusers==0.36.0.dev0) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from diffusers==0.36.0.dev0) (0.7.0)\n",
      "Requirement already satisfied: Pillow in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from diffusers==0.36.0.dev0) (12.0.0)\n",
      "Requirement already satisfied: anyio in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from httpx<1.0.0->diffusers==0.36.0.dev0) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from httpx<1.0.0->diffusers==0.36.0.dev0) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from httpx<1.0.0->diffusers==0.36.0.dev0) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from httpx<1.0.0->diffusers==0.36.0.dev0) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->diffusers==0.36.0.dev0) (0.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.36.0.dev0) (2025.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.36.0.dev0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.36.0.dev0) (6.0.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.36.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.36.0.dev0) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<2.0,>=0.34.0->diffusers==0.36.0.dev0) (0.4.6)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from importlib_metadata->diffusers==0.36.0.dev0) (3.23.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from requests->diffusers==0.36.0.dev0) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\miniconda\\envs\\interior-design\\lib\\site-packages (from requests->diffusers==0.36.0.dev0) (2.6.1)\n",
      "Building wheels for collected packages: diffusers\n",
      "  Building wheel for diffusers (pyproject.toml): started\n",
      "  Building wheel for diffusers (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for diffusers: filename=diffusers-0.36.0.dev0-py3-none-any.whl size=4662269 sha256=37efffe569719e2d18dfb0dc9e533cef2b750dff6152a1b8f0f503ceb0698f84\n",
      "  Stored in directory: C:\\Users\\IR - CP 15\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-jiqjncqm\\wheels\\90\\d4\\44\\a58bc00fb405fefb633b0d9d2307f6e3aec6cc1775d82555d3\n",
      "Successfully built diffusers\n",
      "Installing collected packages: diffusers\n",
      "Successfully installed diffusers-0.36.0.dev0\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/huggingface/diffusers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5077d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1b6bdda",
   "metadata": {},
   "source": [
    "Z-Image Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8be6b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b0e0b9ca0d41df88490c13232d6900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/467 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda\\envs\\interior-design\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\IR - CP 15\\.cache\\huggingface\\hub\\models--Tongyi-MAI--Z-Image-Turbo. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c796ad8d4c3f4b83a484b508f7b39266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a591aff6305648569d9ab0b33ee99a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/model-00001-of-00003.safete(â€¦):   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5c3cb6393640339ee3f210155e9da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/model-00003-of-00003.safete(â€¦):   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4dbf2f834e4228b0c1da01dbb7885d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/model-00002-of-00003.safete(â€¦):   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7373044f5b4976b4134eaf8f5ac391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2329b3421c4fc59eb133a5bcfb67a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3001796a2b4145248579c534c0edfa7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef303451e7dc4f779cdd344d241c5dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d3cbbe1983409cb31eb31b5d9362b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c444a076268e4714921855fc9e89b26c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc812ad207343ef8888fb8152afe7cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527e9f537ea549f9b1431da028490ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d207212361bf47bcb0dcb4e998d7cbfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transformer/diffusion_pytorch_model-0000(â€¦):   0%|          | 0.00/9.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff91fe3332ba49238553527359ce2659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transformer/diffusion_pytorch_model-0000(â€¦):   0%|          | 0.00/9.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765f6c3374994855989a260faefecdf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transformer/diffusion_pytorch_model-0000(â€¦):   0%|          | 0.00/4.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292d9bad1cc34b78a98c85e1d046c192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)ion_pytorch_model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56c85a5c5bb42a1a71d9f796e6adfc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/805 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5f5ab652234da0bfb85882a6054394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/diffusion_pytorch_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5be9d98a16b4335b5179b023618d456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c414f573601447dc9b3b42c10eed42b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a94e18c27fd4b219ea673317045fdc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ZImagePipeline {\n",
       "  \"_class_name\": \"ZImagePipeline\",\n",
       "  \"_diffusers_version\": \"0.36.0.dev0\",\n",
       "  \"_name_or_path\": \"Tongyi-MAI/Z-Image-Turbo\",\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"FlowMatchEulerDiscreteScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"Qwen3Model\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"Qwen2Tokenizer\"\n",
       "  ],\n",
       "  \"transformer\": [\n",
       "    \"diffusers\",\n",
       "    \"ZImageTransformer2DModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import ZImagePipeline\n",
    "\n",
    "# 1. Load the pipeline\n",
    "# Use bfloat16 for optimal performance on supported GPUs\n",
    "pipe = ZImagePipeline.from_pretrained(\n",
    "    \"Tongyi-MAI/Z-Image-Turbo\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=False,\n",
    ")\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "# [Optional] Attention Backend\n",
    "# Diffusers uses SDPA by default. Switch to Flash Attention for better efficiency if supported:\n",
    "# pipe.transformer.set_attention_backend(\"flash\")    # Enable Flash-Attention-2\n",
    "# pipe.transformer.set_attention_backend(\"_flash_3\") # Enable Flash-Attention-3\n",
    "\n",
    "# [Optional] Model Compilation\n",
    "# Compiling the DiT model accelerates inference, but the first run will take longer to compile.\n",
    "# pipe.transformer.compile()\n",
    "\n",
    "# [Optional] CPU Offloading\n",
    "# Enable CPU offloading for memory-constrained devices.\n",
    "# pipe.enable_model_cpu_offload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ddaac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.save_pretrained(\"ZImageModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f85e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2d5a08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bacd915982e4e198bf3b7b5f7976dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "prompt = \"\"\"Generate a photorealistic interior design of a 12x15ft room in Turkish style.\n",
    "Place a dark-wood carved 3-seater sofa centered on the south wall,\n",
    "elegant embroidered curtains on a large west-facing window,\n",
    "a traditional wooden dining table place on left-center space.\n",
    "Use rich red & gold textiles, mosaic lamps, warm lighting,\n",
    "finely patterned Persian carpet, wall decor with Middle Eastern patterns.\n",
    "Ultra realistic, high detail, 4K.\"\"\"\n",
    "\n",
    "# 2. Generate Image\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    num_inference_steps=9,  # This actually results in 8 DiT forwards\n",
    "    guidance_scale=0.0,     # Guidance should be 0 for the Turbo models\n",
    "    generator=torch.Generator(\"cuda\").manual_seed(42),\n",
    ").images[0]\n",
    "\n",
    "image.save(\"interior_test1.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe032084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00e1088c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3784b9bbcb47f190732aa757504c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "prompt = \"\"\"A futuristic rocket performing a precise vertical landing on the surface of Mars. Red Martian dust clouds rise around the landing legs as they touch down. The sky is a soft orange-pink hue with faint sunlight on the horizon. The terrain is rocky and detailed with dunes and craters. The rocket has glowing blue thrusters and sleek metallic panels. Ultra-realistic, high detail, cinematic lighting, NASA-style scientific realism.\"\"\"\n",
    "\n",
    "# 2. Generate Image\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    num_inference_steps=9,  # This actually results in 8 DiT forwards\n",
    "    guidance_scale=0.0,     # Guidance should be 0 for the Turbo models\n",
    "    generator=torch.Generator(\"cuda\").manual_seed(42),\n",
    ").images[0]\n",
    "\n",
    "image.save(\"test2.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "548e2b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6086a777cb3b487ea0e71b647a6f8d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "prompt = \"\"\"A Russian woman walking along a peaceful sandy beach during golden hour. She has light brown hair blowing in the gentle sea breeze, wearing a flowing summer dress. Soft waves roll onto the shore, and warm sunlight reflects off the water. Her footprints trail behind her in the wet sand. The scene is calm, cinematic, and natural, with realistic lighting and detailed textures.\"\"\"\n",
    "\n",
    "# 2. Generate Image\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    num_inference_steps=9,  # This actually results in 8 DiT forwards\n",
    "    guidance_scale=0.0,     # Guidance should be 0 for the Turbo models\n",
    "    generator=torch.Generator(\"cuda\").manual_seed(42),\n",
    ").images[0]\n",
    "\n",
    "image.save(\"test2.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dce335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "751be3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df20f76569b4bc8a01a7318eec1dd2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "prompt = \"\"\"â€œA lone samurai standing in a misty Japanese forest, wearing traditional armor with intricate metal plates and a flowing red sash. He grips a katana reflecting soft morning light. Cherry blossom petals drift through the air. The atmosphere is dramatic and cinematic, with detailed textures, moody lighting, and a sense of ancient honor and discipline.\"\"\"\n",
    "\n",
    "# 2. Generate Image\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    num_inference_steps=9,  # This actually results in 8 DiT forwards\n",
    "    guidance_scale=0.0,     # Guidance should be 0 for the Turbo models\n",
    "    generator=torch.Generator(\"cuda\").manual_seed(42),\n",
    ").images[0]\n",
    "\n",
    "image.save(\"test3.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51e7979a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402cf6b6e48649e6907081cfd0cb9cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "prompt = \"\"\"Photorealistic interior of a bright and airy living room, 12ft x 15ft. A large, comfortable, light-grey linen sofa is placed against the longer 15ft wall, centered. Three armchairs are arranged in a semi-circle facing the sofa: Two matching mid-century modern armchairs in a warm beige velvet, and a single accent chair in textured teal. A large window is centered on the opposite 15ft wall from the sofa, showcasing a clear blue sky with fluffy white clouds. Natural light floods the room. The floor is light oak hardwood with a neutral-toned area rug defining the seating area. Add a simple, rectangular coffee table in a light wood finish in front of the sofa. Walls are painted a soft, warm white. Accessorize with neutral-toned throw pillows on the sofa, a floor lamp next to one armchair, and a leafy green potted plant near the window. 4K, wide-angle lens, detailed textures. The mood is calm and inviting. Soft, diffused natural light.\"\"\"\n",
    "\n",
    "# 2. Generate Image\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    num_inference_steps=9,  # This actually results in 8 DiT forwards\n",
    "    guidance_scale=0.0,     # Guidance should be 0 for the Turbo models\n",
    "    generator=torch.Generator(\"cuda\").manual_seed(42),\n",
    ").images[0]\n",
    "\n",
    "image.save(\"test4.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b8c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image(prompt:str, path):\n",
    "    image = pipe(\n",
    "    prompt=prompt,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    num_inference_steps=9,  # This actually results in 8 DiT forwards\n",
    "    guidance_scale=0.0,     # Guidance should be 0 for the Turbo models\n",
    "    generator=torch.Generator(\"cuda\").manual_seed(42),\n",
    ").images[0]\n",
    "\n",
    "image.save(path)\n",
    "\n",
    "return {\"Image generated and Saved:\": image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea34c87e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2752db28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interior-design",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
